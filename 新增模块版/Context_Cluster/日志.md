```shell
git add . 
git commit -m ""
git push 
```

10/10日 

V1 版本 成功将 由上自下 与 由下至上 通道加入网络中，但是输出的是 每个patch的得分 进而选择 在进行融合 

10/11日

train 文件上传 里面的学习率等参数全部调低了 并非原版

V2 版本 仅仅加入了GCN模块 可以直接运行 参数量较大 在服务器169跑

 损失Nan了  

解决方案1: 猜测是adamw开启了混合精度  尝试加入 eps=1e-4  默认是1e-8  目前来看 确实是这个问题 还没有跑NAN  正在服务器171上跑

10/15日

加入Cluster_visualize 代码 这是可视化的代码 但是需要进行修改 因为模型构造的是1000分类 有两个思路应该 在构造模型的时候就指定是2000分类  或者将最后一个类别改为2000类别 本代码采用的是后面的方法

10/18日

在V2版本基础上，引入残差结构 和新的可训练权重α2 记作V2_1 在服务器165上跑起来   **效果不显著 停止了**

在V3版本基础上，尝试加入注意力部分 在服务器169跑起来哈 得先看懂新文章代码

查看原本Context Cluster是如何进行最后一步特征融合的 我们把选择器加上去



---

10/18日

确定batch_size为64  这样显存为13325 可以在每一台服务器上跑

以下服务器聚类算法统一命名为 Coc

105 >>   跑最基本的 baseline                                        10/18日

106 >>   跑只加了图卷积的模块                                    10/18日

111 >>   跑加入了残差结构的106模块                          10/18日 

112  >>  计划是考虑将注意力加入到图里面来

以上想法交给 崔完成

不要取batch为64





10/23日

context_clusterV3.py 加入了SEnet

在169 服务器 执行的命令 需要对比文件 train83.py

```shell
nohup python3 -u  train.py --data_dir /home/Food2k --model coc_tiny -b 256 --lr 1e-3 --drop-path 0.1 --num-classes 2000 --amp --epochs 300 > 1691023.txt 2>&1

```





10/24日

将华为提出的前半部分模块改造为特征选择模块

后半部分就用GCN模块 目前这个模块正在验证其合理性 好像没什么问题 改改代码直接拼起来就行



10/25日 

实现24日设想的想法并命名为 gold_com.py



10/26日

将PGA模块应用至聚类中心 增强表示 Pga_cluster.py

**在111上服务器重新跑一个esp=1e-4 的实验！！！注意将train.py文件重置**

```
cd /home/
conda activate cluster
nohup python3 -u  train.py --data_dir /home/Food2k --model coc_tiny -b 128 --lr 1e-3 --drop-path 0.1 --num-classes 2000 --amp --opt-eps 1e-4 > origin_clip.txt 2>&1
```

10/27日

PGA加入 三种方案

1. 仅仅增强聚类中心 **还得考虑归一化**   **还有邻居数**

2. 仅仅增强聚类中心映射值 **还得考虑归一化 ** **还有邻居数**

3. 同时增强中心及其映射值   **还有邻居数**

   

服务器165

```python
# Pga_ClusterV1.py 
nohup python3 -u  train.py --data_dir /home/Food2k --model coc_tiny -b 128 --lr 1e-3 --drop-path 0.1 --num-classes 2000 --amp --epochs 300 --opt-eps 1e-4 > PGAV1.txt 2>&1 #  邻居数为8
```





10/28日

将gold_com实现起来

注意执行命令的模型不是 coc_tiny 而是自己的！

> 自动跳转是下载Pylance插件

服务器 112

```python
# Gold_com.py
nohup python3 -u  train.py --data_dir /home/Food2k --model coc_tiny -b 128 --lr 1e-3 --drop-path 0.1 --num-classes 2000 --amp --epochs 300 --opt-eps 1e-4 >Gold_Com.txt 2>&1
```

以上选的是tiny模型  文件的789行开始这些参数都是tiny的 到时候得修改 今天先跑小的





10/29日



服务器训练汇总

- [x] 165 Pga_ClusterV1.py  只加入PGA模块
- [x] 169 在171基础上加入了通道注意力  感觉没必要  而且batch_size是256  **暂停了哈**
- [x] 171 只加入了Com模块  日志文件coc_gcnv2.txt 
- [ ] 105
- [ ] 106
- [x] 111  原始骨干用来对比 加入训练参数 1e-4 
- [x] 112  Gold_com.py           只加入Gold_Com 模块





**接下安排是**

1.将模块合并起来看效果    >>>  在106上命名为finnal.py 但是可能由于显存不够 导致咱这个不太行 到时候换一下

2.寻找一些小的训练技巧 将他们在baseline上除去  >>>  在104-106三台服务器都尝试一下



10/30日

将169服务器停止 先将他们拼凑起来 169配置为 D:\桌面\Food2K论文相关材料\1691023.txt 此文件包含了信息执行命令是

```python
nohup python3 -u  train.py --data_dir /home/Food2k --model coc_tiny -b 256 --lr 1e-3 --drop-path 0.1 --num-classes 2000 --amp --epochs 300 > 1691023.txt 2>&1
```



开启finnal.py 篇章！ >> 服务器169

```
nohup python3 -u  train.py --data_dir /home/Food2k --model coc_tiny -b 256 --lr 1e-3 --drop-path 0.1 --num-classes 2000 --amp --epochs 300 --opt-eps 1e-4 >Finnal.txt 2>&1
```





105重设种子

```
nohup python3 -u  train.py --data_dir /home/Food2k --model coc_tiny -b 128 --lr 1e-3 --drop-path 0.1 --num-classes 2000 --amp --opt-eps 1e-4 > origin_clipseed_36.txt 2>&1
```



服务器训练汇总

- [x] 165 Pga_ClusterV1.py  只加入PGA模块
- [x] 169 在171基础上加入了通道注意力  感觉没必要  而且batch_size是256  **暂停了哈** 改为Finnal新篇章
- [x] 171 只加入了Com模块  日志文件coc_gcnv2.txt **暂停**  模型是Com_Cluser.py

```
nohup python3 -u  train.py --data_dir /home/Food2k --model coc_tiny -b 128 --lr 1e-3 --drop-path 0.1 --num-classes 2000 --amp --epochs 300 --opt-eps 1e-4 > coc_gcnv2.txt 2>&1
```



- [x] 105 105重设种子
- [x] 106  设了一个batch=64的实验  **11/1日停止**  模型是Select_Cluster

```
nohup python3 -u  train.py --data_dir /home/Food2k --model coc_tiny -b 64 --lr 1e-3 --drop-path 0.1 --num-classes 2000 --amp --epochs 300 --opt-eps 1e-4 > Coc_batch64.txt 2>&1
```

- [x] 111  原始骨干用来对比 加入训练参数 1e-4 
- [x] 112  Gold_com.py           只加入Gold_Com 模块





11/1日

些许总结

加入eps=1e-4  貌似要比同期的好

目前来看 PGA方案1要比baseline效果好 目前一直领先



现在就是思考特征融合方法

Gold_Com的Gold模块 这个是针对Yolo系列的 这个系列对推理时长是有要求的 而其主体RepVGGBlock就是多分支训练，合并分支推理 不太好！

服务器171 EVC_Cluster.py

```
nohup python3 -u  train.py --data_dir /home/Food2k --model coc_tiny -b 128 --lr 1e-3 --drop-path 0.1 --num-classes 2000 --amp --epochs 300 --opt-eps 1e-4 > EVC_Cluster.txt 2>&1
```

 服务器169停止



11/2日

服务器169

```
nohup python3 -u  train.py --data_dir /home/Food2k --model coc_tiny -b 128 --lr 1e-3 --drop-path 0.1 --num-classes 2000 --amp --epochs 300 --opt-eps 1e-4 >FinnalV2.txt 2>&1
```

同比效果也不好 

接下来三个方案 LVC + FPN + Com

或者是 AFPN + Com

FPN + Com

看看效果



11/3日
EVC_Cluster 宣布不行 重新思考  v2版本





11/4日

服务器汇总

- [x] 165 Pga_ClusterV1.py  只加入PGA模块  
- [x] 169 在171基础上加入了通道注意力  感觉没必要  而且batch_size是256  **暂停了哈** 改为Finnal新篇章
- [x] 171 只加入了Com模块  日志文件coc_gcnv2.txt **暂停**  模型是Com_Cluser.py

```
nohup python3 -u  train.py --data_dir /home/Food2k --model coc_tiny -b 128 --lr 1e-3 --drop-path 0.1 --num-classes 2000 --amp --epochs 300 --opt-eps 1e-4 > coc_gcnv2.txt 2>&1
```



- [x] 105 105重设种子
- [x] 106  设了一个batch=64的实验  **11/1日停止**  模型是Select_Cluster

```
nohup python3 -u  train.py --data_dir /home/Food2k --model coc_tiny -b 64 --lr 1e-3 --drop-path 0.1 --num-classes 2000 --amp --epochs 300 --opt-eps 1e-4 > Coc_batch64.txt 2>&1
```

- [x] 111  原始骨干用来对比 加入训练参数 1e-4 
- [x] 112  Gold_com.py           只加入Gold_Com 模块









**165 PGA**

**169 融合模块**

**171 总模型**

**111 对比模型**

